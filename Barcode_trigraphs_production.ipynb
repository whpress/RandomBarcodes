{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful hack: do this to restart the kernel (and clear cuda memory) from within the notebook\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRELIMINARIES Step 0: Needed for all additional steps, define functions used:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import pickle\n",
    "import torch\n",
    "import re\n",
    "\n",
    "alphabet = \"acgt\"\n",
    "def decode(x): # convert list of ints to string\n",
    "    s = \"\".join([alphabet[xx] for xx in x])\n",
    "    return s\n",
    "def encode(st): # convert a string into a list of ints\n",
    "    x = [alphabet.index(ss) for ss in st]\n",
    "    return x\n",
    "def seqtomer(seq) : # return list of trimers in a seq\n",
    "    ans = [int(16*seq[k] + 4*seq[k+1] + seq[k+2]) for k in range(len(seq)-2)]\n",
    "    return ans\n",
    "def mertobin(mer) : # trimer list to occupancy uint64 bitmap\n",
    "    ibin = 0\n",
    "    for m in mer :\n",
    "        ibin |= (1 << m)\n",
    "    return ibin\n",
    "\n",
    "def makeerrors(seq,srate,irate,drate) : #error mode applied to a sequence seq\n",
    "    # note: modifies (also returns) seq\n",
    "    n = len(seq)\n",
    "    # substitutions\n",
    "    ns = random.binomial(n,srate*1.3333) # 3/4 of substitutions are \"effective\"\n",
    "    ndx = random.randint(low=0,high=n,size=ns)\n",
    "    vals = random.randint(low=0,high=4,size=ns)\n",
    "    seq[ndx] = vals\n",
    "    # deletions\n",
    "    nd = random.binomial(n,drate)\n",
    "    ndx = random.randint(low=0,high=n,size=nd)\n",
    "    seq = delete(seq,ndx)    \n",
    "    # insertions (at specified rate into smaller seq)\n",
    "    ni = random.binomial(len(seq),irate)\n",
    "    ndx = random.randint(low=0,high=len(seq)+1,size=ni)\n",
    "    vals = random.randint(low=0,high=4,size=ni)\n",
    "    seq = insert(seq,ndx,vals)\n",
    "    # pad or truncate to original length\n",
    "    nn = len(seq)\n",
    "    if nn > n :\n",
    "        seq = seq[:n]\n",
    "    elif nn < n :\n",
    "        seq = concatenate((seq,random.randint(low=0,high=4,size=n-nn)))\n",
    "    return seq\n",
    "\n",
    "m0 = uint64(0x5555555555555555)  # binary: 0101...\n",
    "m1 = uint64(0x3333333333333333)  # binary: 00110011..\n",
    "m2 = uint64(0x0f0f0f0f0f0f0f0f)  # binary:  4 zeros,  4 ones ...\n",
    "m3 = uint64(0x00ff00ff00ff00ff)  # binary:  8 zeros,  8 ones ...\n",
    "m4 = uint64(0x0000ffff0000ffff)  # binary: 16 zeros, 16 ones ...\n",
    "m5 = uint64(0x00000000ffffffff)  # binary: 32 zeros, 32 ones\n",
    "def popcount(x):\n",
    "# https://github.com/google/jax/blob/6c8fc1b031275c85b02cb819c6caa5afa002fa1d/jax/lax_reference.py#L121-L150\n",
    "    x = (x & m0) + ((x >>  1) & m0)  # put count of each  2 bits into those  2 bits\n",
    "    x = (x & m1) + ((x >>  2) & m1)  # put count of each  4 bits into those  4 bits\n",
    "    x = (x & m2) + ((x >>  4) & m2)  # put count of each  8 bits into those  8 bits\n",
    "    x = (x & m3) + ((x >>  8) & m3)  # put count of each 16 bits into those 16 bits\n",
    "    x = (x & m4) + ((x >> 16) & m4)  # put count of each 32 bits into those 32 bits\n",
    "    x = (x & m5) + ((x >> 32) & m5)  # put count of each 64 bits into those 64 bits\n",
    "    return x\n",
    "\n",
    "try :\n",
    "    from popcll_torch import popcll\n",
    "    #https://github.com/iamgroot42/popcll_torch , thanks to Anshuman Suri (as9rw@virginia.edu)!\n",
    "    mypopcount = popcll\n",
    "except :\n",
    "    mypopcount = popcount\n",
    "    print(\"popcll_torch not found, so using slower popcount\")\n",
    "\n",
    "def find_runs(x):\n",
    "    #Find runs of consecutive items in an array.\n",
    "    # credit: https://gist.github.com/alimanfoo/c5977e87111abe8127453b21204c1065\n",
    "    n = x.shape[0]\n",
    "    # find run starts\n",
    "    loc_run_start = empty(n, dtype=bool)\n",
    "    loc_run_start[0] = True\n",
    "    not_equal(x[:-1], x[1:], out=loc_run_start[1:])\n",
    "    run_starts = nonzero(loc_run_start)[0]\n",
    "    # find run values\n",
    "    run_values = x[loc_run_start]\n",
    "    # find run lengths\n",
    "    run_lengths = diff(append(run_starts, n))\n",
    "    return run_values, run_starts, run_lengths\n",
    "\n",
    "def chemfilter(seq, homomax=3, atmax=22, cgmax=22) :\n",
    "    # returns whether seq satisfies chemistry constraints\n",
    "    bc = bincount(seq,minlength=4)\n",
    "    if (bc[0]+bc[3] > atmax) or (bc[1]+bc[2] > cgmax) : return False\n",
    "    _,_,run_lengths = find_runs(seq)\n",
    "    if max(run_lengths) > homomax : return False\n",
    "    return True\n",
    "\n",
    "def allcoses(mer, tcosvecs) : # correlate a mer against all the cosine templates\n",
    "    mmer = torch.LongTensor(mer).to(device)\n",
    "    ncos = tcosvecs.shape[0]\n",
    "    cosvec = torch.zeros(ncos, 64, dtype=torch.float, device=device)\n",
    "    for k in range(ncos) :\n",
    "        source = tcoses[k, torch.arange(len(mmer), dtype=torch.long, device=device)]\n",
    "        cosvec[k,:].index_add_(0,mmer,source)\n",
    "    return torch.sum(torch.unsqueeze(cosvec,dim=1)*tcosvecs,dim=2)\n",
    "\n",
    "def prank(arr, descending=False) : # returns rank of each element in torch array\n",
    "    argsrt = torch.argsort(arr, descending=descending)\n",
    "    rank = torch.zeros(arr.shape, dtype=torch.float, device=device)\n",
    "    rank[argsrt] = torch.arange(len(argsrt),dtype=torch.float,device=device)\n",
    "    return rank    \n",
    "\n",
    "class ApproximateLevenshtein :\n",
    "    def __init__(s, M, N, Q, zsub, zins, zdel, zskew):\n",
    "        torch.set_grad_enabled(False) # just in case not done elsewhere!\n",
    "        s.M = M # length of seq1\n",
    "        s.N = N # length of each seq2\n",
    "        s.Q = Q # number of seq2s\n",
    "        (s.zsub, s.zins, s.zdel, s.zskew) = (zsub, zins, zdel, zskew)\n",
    "        s.tab = torch.zeros(N+1,Q, device=device)\n",
    "        \n",
    "    def __call__(s,seq1,seq2) :\n",
    "        assert (len(seq1) == s.M) and (seq2.shape[1] == s.N) and (seq2.shape[0] == s.Q)\n",
    "        s.tab[:,:] = (s.zskew * torch.arange(s.N+1., device=device)).unsqueeze(1) # force broadcast\n",
    "        for i in range(1,s.M+1) :\n",
    "            diag = s.tab[:-1,:] + torch.where(seq1[i-1] == seq2.t(), 0., s.zsub) # diagonal move\n",
    "            s.tab[0,:] += s.zskew\n",
    "            s.tab[1:,:] += s.zdel # down move\n",
    "            s.tab[1:,:] = torch.minimum(s.tab[1:,:], diag) # or diag if better\n",
    "            s.tab[1:,:] = torch.minimum(s.tab[1:,:], s.tab[:-1,:] + s.zins) # right move\n",
    "            s.tab[1:,:] = torch.minimum(s.tab[1:,:], s.tab[:-1,:] + s.zins) # repeat (>= 0 times) as you can afford\n",
    "           # N.B.: M >= N gives better approx than N > M, so change arg order accordingly\n",
    "        return s.tab[s.N,:]\n",
    "\n",
    "class ParallelLevenshtein :\n",
    "    def __init__(s, M, N, Q, zsub, zins, zdel, zskew):\n",
    "        torch.set_grad_enabled(False) # just in case not done elsewhere!\n",
    "        s.M = M # length of seq1\n",
    "        s.N = N # length of each seq2\n",
    "        s.Q = Q # number of seq2s\n",
    "        (s.zsub, s.zins, s.zdel, s.zskew) = (zsub, zins, zdel, zskew)\n",
    "        MN1 = M + N + 1\n",
    "        s.blue = torch.zeros(Q, MN1, MN1, device=device)\n",
    "        s.bluef = s.blue.view(Q, MN1 * MN1)\n",
    "        s.ndxr = torch.zeros(M*N, dtype=int, device=device) # index of mer matches array into flat blue\n",
    "        for m in torch.arange(M,device=device) :\n",
    "            for n in torch.arange(N,device=device) :\n",
    "                s.ndxr[n + N*m] = (3*M+2*N+2) + (M+N)*m + (M+N+2)*n\n",
    "        s.lls = torch.zeros(MN1+1,dtype=torch.int,device=device)       \n",
    "        s.rrs = torch.zeros(MN1+1,dtype=torch.int,device=device)       \n",
    "        for i in range(2,MN1+1) :\n",
    "            s.lls[i] = abs(M - i + 1) + 1\n",
    "            s.rrs[i] = (M+N-1) - abs(- i + 1 + N )\n",
    "\n",
    "    def __call__(s, seq1, sseq2): # single seq1, tensor of sseq2s\n",
    "        assert (len(seq1) == s.M) and (sseq2.shape[1] == s.N) and (sseq2.shape[0] == s.Q)    \n",
    "        (M1,N1,MN,MN1,MN2) = (s.M + 1, s.N + 1, s.M + s.N, s.M + s.N + 1, s.M + s.N + 2)\n",
    "        abmatch = (seq1.view(1,s.M,1) != sseq2.view(s.Q,1,s.N)).type(torch.float) * s.zsub\n",
    "        s.bluef[:,s.ndxr] = abmatch.view(s.Q,s.M*s.N)\n",
    "        s.bluef[:,torch.arange(s.M,MN2*N1,MN2)] = (s.zskew*torch.arange(N1,device=device)).unsqueeze(0)\n",
    "        s.bluef[:,torch.arange(s.M,MN*M1,MN)] = (s.zskew*torch.arange(M1,device=device)).unsqueeze(0)\n",
    "        for k in torch.arange(2,MN1,device=device) :\n",
    "            ll = s.lls[k]\n",
    "            rr = s.rrs[k]\n",
    "            slis = torch.arange(ll,rr+1,2,device=device)\n",
    "            s.blue[:,k,slis] = torch.minimum(\n",
    "                s.blue[:,k,slis] + s.blue[:,k-2,slis],\n",
    "                torch.minimum(\n",
    "                    s.blue[:,k-1,slis-1] + s.zdel,\n",
    "                    s.blue[:,k-1,slis+1] + s.zins\n",
    "                )\n",
    "            )\n",
    "        return s.blue[:,-1,s.N]\n",
    "\n",
    "print(\"all functions now defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output above is\n",
    "`all functions now defined.`\\\n",
    "If you see `popcll_torch not found, so using slower popcount`\n",
    "then you may want to install popcll_torch from\n",
    "[here](https://github.com/iamgroot42/popcll_torch)\n",
    "for a modest speed improvement and smaller GPU memory use\n",
    "\n",
    "Next: **PRELIMINARIES: Do either Step 1a *or* Step 1b** to create a barcode library or use an existing (saved) one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Step 1a: create to a set of random codewords (do just once)\n",
    "# writes to 2 files: ascii acgt strings, and (with auxilliary tables) Python pickle  \n",
    "\n",
    "N = 100000 # user to set to number of codewords (this is a small value for demo purposes, 1000000 is feasible)\n",
    "M = 34 # user to set to length of a codeword (nt)\n",
    "filename = \"/tmp/randomcode_wchem_1e5_34\" # user set to output file path (produces file length ~ M*N)\n",
    "picklefilename = \"/tmp/randomcode_wchem_1e5_34.pkl\" # user to set (produces file length ~64*M*N)\n",
    "Ncos = 4 # user set to number of cosine templates (usually 4)\n",
    "DoChemFilter = True # filter for homopolymer runs, AT and CG content?\n",
    "(homomax, atmax, cgmax) = (3, int(0.66*M), int(0.66*M)) # user to set if DoChemFilter is True\n",
    "\n",
    "# generate and write the codes\n",
    "torch.set_grad_enabled(False)\n",
    "if DoChemFilter :\n",
    "    fac = 2. # should work in all expected cases\n",
    "    Ntrial = int(fac*N)\n",
    "    allcands = random.randint(low=0,high=4,size=(Ntrial,M))\n",
    "    goodcands = array([chemfilter(x,homomax,atmax,cgmax) for x in allcands], dtype=bool)\n",
    "    allcodes = allcands[goodcands][:N] # if this or next throws an error, increase fac, but shouldn't happen\n",
    "    if allcodes.shape != (N,M) : raise RuntimeError(\"see code, increase fac (this shouldn\\'t happen')\")\n",
    "else :\n",
    "    allcodes = random.randint(low=0,high=4,size=(N,M))\n",
    "with open(filename, 'w') as OUT:\n",
    "    for code in allcodes :        \n",
    "        OUT.write(decode(code) + '\\n')\n",
    "print(f\"wrote {N} codewords of length {M} in ascii to {filename}, now computing auxilliary tables\")\n",
    "\n",
    "# re-read codes to be sure that the pickle file matches the ascii file!\n",
    "with open(filename) as IN: \n",
    "    codes=IN.readlines() # have \\n EOL\n",
    "assert N == len(codes)\n",
    "assert M == len(codes[0][:-1])\n",
    "allseqs = []\n",
    "alltrimers = []\n",
    "allbitmaps = zeros(N, dtype=uint64)\n",
    "cosvecs = torch.zeros((Ncos,N,64),dtype=torch.float)\n",
    "coses = zeros((Ncos,M)) \n",
    "for k in range(Ncos) :\n",
    "    coses[k,:] = cos(pi*arange(M)*(k+1.)/(M-1.))\n",
    "tcoses = torch.tensor(coses, dtype=torch.float)\n",
    "for i,code in enumerate(codes) :\n",
    "    seq = encode(code[:-1])\n",
    "    allseqs.append(seq)\n",
    "    mer = seqtomer(seq)\n",
    "    mmer = torch.LongTensor(mer)\n",
    "    alltrimers.append(mer)\n",
    "    allbitmaps[i] = mertobin(mer)\n",
    "    for k in range(Ncos):\n",
    "        source = tcoses[k,arange(M-2)]\n",
    "        cosvecs[k,i,:].index_add_(0,mmer,source)\n",
    "print(\"finished making code auxilliary tables, now pickling\")        \n",
    "pickledict = {\"N\" : N, \"M\" : M, \"allseqs\" : allseqs,\n",
    "    \"alltrimers\" : alltrimers, \"allbitmaps\" : allbitmaps, \"coses\" : coses, \"cosvecs\" : cosvecs}\n",
    "with open(picklefilename,'wb') as OUT :\n",
    "    pickle.dump(pickledict,OUT)\n",
    "print(f\"finished pickling code with {N} codewords of length {M} to {picklefilename}\")        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical output above is\\\n",
    "`wrote 100000 codewords of length 34 in ascii to /tmp/randomcode_wchem_1e5_34, now computing auxilliary tables\n",
    "finished making code auxilliary tables, now pickling\n",
    "finished pickling code with 100000 codewords of length 34 to /tmp/randomcode_wchem_1e5_34.pkl\n",
    "CPU times: user 5.43 s, sys: 326 ms, total: 5.76 s\n",
    "Wall time: 5.77 s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# *or* Step 1b: load code from its pickle file\n",
    "\n",
    "picklefilename = \"/tmp/randomcode_wchem_1e5_34.pkl\" # user to set\n",
    "with open(picklefilename,'rb') as IN :\n",
    "    pickledict = pickle.load(IN)\n",
    "(N, M, allseqs, alltrimers, allbitmaps, coses, cosvecs) = \\\n",
    "    [pickledict[x] for x in ('N', 'M', 'allseqs', 'alltrimers', 'allbitmaps', 'coses', 'cosvecs')]\n",
    "print(f\"loaded code with {N} codewords of length {M} from {picklefilename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical output above is\\\n",
    "`loaded code with 100000 codewords of length 34 from /tmp/randomcode_wchem_1e5_34.pkl.\n",
    "CPU times: user 372 ms, sys: 95.9 ms, total: 468 ms\n",
    "Wall time: 464 ms`\n",
    "\n",
    "**Next: PRELIMINARIES Step 2, make simulated reads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# PRELIMINARIES Step 2: make simulated data, write \"reads\" to ascii file,\n",
    "# \"answers (indices of true codeword)\" to another file. Assumes code is loaded by Step 1a or 1b.\n",
    "\n",
    "Q = 10000 # user to set to desired number of simulated reads (10000 is a small value for demo purposes)\n",
    "nave = 4. # user to set (average Poisson number of reads from each randomly selected codeword)\n",
    "(srate,irate,drate) = (0.05, 0.05, 0.10) # user to set (these are *very* large error rates)\n",
    "#(srate,irate,drate) = (0.0333, 0.0333, 0.0333) # user to set (these are large error rates)\n",
    "readsfilename = \"/tmp/randomcode_1e6_34_sim_reads\" # user set to output file path\n",
    "answersfilename = \"/tmp/randomcode_1e6_34_sim_answers\" # user set to output file path\n",
    "\n",
    "q = 0\n",
    "reads = []\n",
    "answers = []\n",
    "while q < Q :\n",
    "    ansindex = random.randint(low=0,high=N)\n",
    "    ans = allseqs[ansindex]\n",
    "    n = min(Q-q, random.poisson(lam=nave))\n",
    "    for i in range(n) :\n",
    "        reads.append(makeerrors(copy(ans),srate,irate,drate))\n",
    "        answers.append(ansindex)\n",
    "    q += n\n",
    "with open(readsfilename, 'w') as OUT:\n",
    "    for seq in reads :        \n",
    "        OUT.write(decode(seq) + '\\n')\n",
    "with open(answersfilename, 'w') as OUT:\n",
    "    for ans in answers :        \n",
    "        OUT.write(str(ans) + '\\n')\n",
    "print(f\"done creating {Q} simulated reads and writing to {readsfilename} \\n(answers to {answersfilename})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical output above is\\\n",
    "`done creating 10000 simulated reads and writing to /tmp/randomcode_1e6_34_sim_reads\n",
    "(answers to /tmp/randomcode_1e6_34_sim_answers)\n",
    "CPU times: user 766 ms, sys: 19.2 ms, total: 785 ms\n",
    "Wall time: 774 ms`\n",
    "\n",
    "**Code creation and simulated data steps complete. Now demonstrate decoding (as if from data):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# DECODING Step 1. Move the code (assumed loaded above) and reads (from file) to the GPU.\n",
    "\n",
    "readsfilename = \"/tmp/randomcode_1e6_34_sim_reads\" # user to set\n",
    "\n",
    "if torch.cuda.is_available() :\n",
    "    device = torch.device(\"cuda\")\n",
    "    cudaname = torch.cuda.get_device_name()\n",
    "else :\n",
    "    raise RuntimeError(\"Required GPU not found! Exiting.\")\n",
    "print(f\"Using {device} device {cudaname}.\")\n",
    "\n",
    "with open(readsfilename, 'r') as IN:\n",
    "    stringreads=IN.readlines()\n",
    "reads = []\n",
    "for read in stringreads :\n",
    "    reads.append(encode(read[:-1])) # lose the \\n\n",
    "reads = array(reads)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "tallseqs = torch.tensor(array(allseqs), device=device)\n",
    "talltrimers = torch.tensor(array(alltrimers), device=device)\n",
    "tallbitmaps = torch.tensor(allbitmaps.astype(int64), dtype=torch.int64, device=device) # \n",
    "tcoses = torch.tensor(coses, dtype=torch.float, device=device)\n",
    "tcosvecs = cosvecs.to(device)\n",
    "print(f\"found {reads.shape[0]} reads of length {reads.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical output above is\\\n",
    "`Using cuda device NVIDIA GeForce RTX 3090.\n",
    "found 10000 reads of length 34\n",
    "CPU times: user 1.18 s, sys: 429 ms, total: 1.61 s\n",
    "Wall time: 1.46 s`\n",
    "\n",
    "**Next: the main decoding step, with almost all the running time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# DECODING Step 2 (main step). Primary and secondary triage, followed by Levenshtein\n",
    "\n",
    "Qq = 10000 # reads.shape[0] or smaller number, how many reads to do\n",
    "Ntriage = 10000 # user set to number passed from triage to Levenshtein\n",
    "Nthresh = 8 # user set to Levenshtein score greater than which is called an erasure\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "mydist = ApproximateLevenshtein(M,M,Ntriage, 1.,1.,1.,1.)\n",
    "#mydist = ParallelLevenshtein(M,M,Ntriage, 1.,1.,1.,1.)\n",
    "\n",
    "Ncos = tcosvecs.shape[0]\n",
    "dists = torch.zeros(Ncos+1, N, dtype=torch.float, device=device) # will hold distances for each read\n",
    "allrank = torch.zeros(Ncos+1 ,N, dtype=torch.float, device=device)\n",
    "best = torch.zeros(Qq, dtype=torch.long, device=device)\n",
    "\n",
    "for j,seq in enumerate(reads[:Qq]) :\n",
    "    # primary and secondary triage\n",
    "    mer = seqtomer(seq)\n",
    "    foo = int64(uint64(mertobin(mer))) # need to cast 64 bits to a type known to torch\n",
    "    seqbin = torch.tensor(foo,dtype=torch.int64,device=device)\n",
    "    xored = torch.bitwise_xor(seqbin,tallbitmaps)\n",
    "    dists[0,:] = 64. - mypopcount(xored) # all Hamming distances\n",
    "    cosvec = torch.zeros(Ncos, 64, dtype=torch.float, device=device)\n",
    "    for k in range(Ncos) :\n",
    "        cosvec[k,mer] =  tcoses[k, torch.arange(len(mer), dtype=torch.long, device=device)]\n",
    "    dists[1:,:] = torch.sum(torch.unsqueeze(cosvec,dim=1)*tcosvecs,dim=2) # all cosine distances\n",
    "    for k in range(Ncos+1) :\n",
    "        allrank[k,:] = prank(dists[k,:], descending=True) # rank them all\n",
    "    offset = 1.\n",
    "    fm = torch.prod(offset+allrank,dim=0)\n",
    "    fmargsort = torch.argsort(fm)\n",
    "    # Levenshtein distance\n",
    "    tseq1 = torch.tensor(seq,device=device)\n",
    "    tseq2 = tallseqs[fmargsort[:Ntriage],:]\n",
    "    ans = mydist(tseq1,tseq2)\n",
    "    ia = torch.argmin(ans) # index into fmargsort of best\n",
    "    ibest = fmargsort[ia] # index of best codeword in codes\n",
    "    best[j] = (ibest if ans[ia] <= Nthresh else -1) # erasures returned as -1\n",
    "\n",
    "print(\"done with decoding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical output above is\\\n",
    "`done with decoding.\n",
    "CPU times: user 50.9 s, sys: 11.5 ms, total: 51 s\n",
    "Wall time: 50.9 s`\n",
    "\n",
    "If we were decoding real data (with the right answers not known) we would be done. The tensor `best` contains the index of the codeword of each read, or -1 if it is an erasure (i.e., not accurately readable). But in simulation we know the right answers and can check them.\n",
    "\n",
    "**Next: Check simulation's recall and precision:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECODING Step 3a (for simulations done within this notebook).\n",
    "# If there is an answers file, compute precision and recall of the decode.\n",
    "\n",
    "answersfilename = \"/tmp/randomcode_1e6_34_sim_answers\" # user set to output file path\n",
    "answers = genfromtxt(answersfilename, dtype=int)\n",
    "best = best.cpu().numpy()\n",
    "ngood = sum(logical_and(answers[:Qq] == best[:Qq], best[:Qq] >= 0))\n",
    "nbad = sum(logical_and(answers[:Qq] != best[:Qq], best[:Qq] >= 0))\n",
    "nerase = sum(best[:Qq] < 0)\n",
    "recall = 1. - nerase / Qq\n",
    "precision = ngood/(ngood + nbad + 1.e-30)\n",
    "\n",
    "print(f\"In {Qq} decodes, {ngood} were correct, {nbad} were wrong, {nerase} were erasures.\")\n",
    "print(f\"precision = {precision:.4f}, recall = {recall:.4f}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the \"very large\" error rates (0.05,0.05,0.10) typical output is\\\n",
    "`In 10000 decodes, 6634 were correct, 5 were wrong, 3361 were erasures.\n",
    "precision = 0.9992, recall = 0.6639`\\\n",
    "For the \"large\" error rates (0.0333,0.0333,0.0333) typical output is\\\n",
    "`In 10000 decodes, 9751 were correct, 1 were wrong, 248 were erasures.\n",
    "precision = 0.9999, recall = 0.9752.`\n",
    "\n",
    "**Next: Checking simulated reads processed by the Python batch file (see Github explanation):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECODING Step 3b (for simulations processed by the standalone Python file barcode_batch.py).\n",
    "from numpy import *\n",
    "import re\n",
    "\n",
    "answersfilename = \"/tmp/randomcode_1e6_34_sim_1e6answers\" # user to set\n",
    "decodefilestem = \"/tmp/batch_sim_output_\" # user to set\n",
    "decodefilesuffixes = [\"1of4\",\"2of4\",\"3of4\",\"4of4\"] # user to set for files spanning answers\n",
    "\n",
    "answers = genfromtxt(answersfilename, dtype=int)\n",
    "totlines = len(answers)\n",
    "print(f\"found {totlines} answers\")\n",
    "\n",
    "for suffix in decodefilesuffixes :\n",
    "    #convert suffixes to line numbers in answer file\n",
    "    num,denom = re.match('([0-9]+)of([0-9]+)',suffix).groups()\n",
    "    startline = int(totlines*(float(num)-1.)/float(denom)) # exactly same as barcode_batch.py\n",
    "    endline = int(totlines*float(num)/float(denom)) # ditto\n",
    "    print(f\"comparing segment {suffix} to answer lines {startline}:{endline}:\")\n",
    "    # read the decodes and compare\n",
    "    decodefile = decodefilestem + suffix\n",
    "    decodes = genfromtxt(decodefile, dtype=int)\n",
    "    ngood = sum(logical_and(answers[startline:endline] == decodes, decodes >= 0))\n",
    "    nbad = sum(logical_and(answers[startline:endline] != decodes, decodes >= 0))\n",
    "    nerase = sum(decodes < 0)\n",
    "    recall = 1. - nerase / len(decodes)\n",
    "    precision = ngood/(ngood + nbad + 1.e-30)\n",
    "    print(f\"     In {len(decodes)} decodes, {ngood} were correct, {nbad} were wrong, {nerase} were erasures.\")\n",
    "    print(f\"     precision = {precision:.4f}, recall = {recall:.4f}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
